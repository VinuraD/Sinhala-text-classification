{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2836ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "\u001b[K     |████████████████████████████████| 264 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 87.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.42\n",
      "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 679 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-5.0.0-cp38-cp38-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.6 MB 139.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 723 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from datasets) (2.24.0)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 810 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2021.05.0\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 90.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 118.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 152.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from datasets) (1.19.1)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 86.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.8.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "\u001b[K     |████████████████████████████████| 761 kB 100.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 39.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "\u001b[K     |████████████████████████████████| 662 kB 154.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.3\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 133.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 145.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, tqdm, regex, pytz, joblib, filelock, dill, xxhash, tokenizers, sacremoses, pyyaml, pyarrow, pandas, multiprocess, huggingface-hub, fsspec, transformers, datasets\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires flatbuffers~=1.12.0, but you have flatbuffers 20210226132247 which is incompatible.\n",
      "tensorflow 2.4.1 requires gast==0.3.3, but you have gast 0.4.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires grpcio~=1.32.0, but you have grpcio 1.36.1 which is incompatible.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.19.1 which is incompatible.\n",
      "tensorflow 2.4.1 requires opt-einsum~=3.3.0, but you have opt-einsum 3.1.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires tensorflow-estimator<2.5.0,>=2.4.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\n",
      "Successfully installed datasets-1.11.0 dill-0.3.4 filelock-3.0.12 fsspec-2021.8.1 huggingface-hub-0.0.16 joblib-1.0.1 multiprocess-0.70.12.2 pandas-1.3.2 pyarrow-5.0.0 pytz-2021.1 pyyaml-5.4.1 regex-2021.8.28 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.62.2 transformers-4.10.0 typing-extensions-3.10.0.2 xxhash-2.0.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9 MB 2.0 MB/s eta 0:00:01    |█████████▍                      | 7.3 MB 2.0 MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1317 sha256=bf99f7aa266d9e09a52036b0dbc211e59fb6ee3aac617255b04b8ba8ae975fe3\n",
      "  Stored in directory: /userdirs/vinura/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-2.2.0\n",
      "Requirement already satisfied: numpy in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (1.19.1)\n",
      "Collecting keras-adamw\n",
      "  Downloading keras_adamw-1.38-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from keras-adamw) (1.19.1)\n",
      "Requirement already satisfied: tensorflow in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from keras-adamw) (2.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (0.37.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 385 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (1.6.3)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (3.17.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (0.13.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (2.10.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (2.5.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorflow->keras-adamw) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 65.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (1.22.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (3.3.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (1.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow->keras-adamw) (0.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-adamw) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-adamw) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-adamw) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->keras-adamw) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-adamw) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-adamw) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-adamw) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-adamw) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-adamw) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /userdirs/vinura/miniconda3/envs/tfenv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->keras-adamw) (3.1.0)\n",
      "Installing collected packages: six, tensorboard-data-server, numpy, grpcio, typing-extensions, tensorflow-estimator, opt-einsum, gast, flatbuffers, keras-adamw\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.36.1\n",
      "    Uninstalling grpcio-1.36.1:\n",
      "      Successfully uninstalled grpcio-1.36.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 3.1.0\n",
      "    Uninstalling opt-einsum-3.1.0:\n",
      "      Successfully uninstalled opt-einsum-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 20210226132247\n",
      "    Uninstalling flatbuffers-20210226132247:\n",
      "      Successfully uninstalled flatbuffers-20210226132247\n",
      "Successfully installed flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 keras-adamw-1.38 numpy-1.19.5 opt-einsum-3.3.0 six-1.15.0 tensorboard-data-server-0.6.1 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.14.0 typeguard-2.12.1\n",
      "Collecting helasentilex\n",
      "  Downloading helasentilex-0.1.1-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 292 kB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: helasentilex\n",
      "Successfully installed helasentilex-0.1.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets transformers\n",
    "#!pip install sentencepiece\n",
    "#!pip install sklearn\n",
    "#!pip install numpy\n",
    "#!pip install keras-adamw\n",
    "#!pip install tensorflow-addons\n",
    "#!pip install helasentilex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d09783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:17:54.113820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import cumsum\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d34487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os; os.environ[\"TF_KERAS\"]='1'\n",
    "#from keras_adamw import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daae4871",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "2.4.1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_393520/858360534.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 17:34:16.037247: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-03 17:34:16.037780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-03 17:34:16.092042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.093026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2021-09-03 17:34:16.093112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-03 17:34:16.095155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-03 17:34:16.095218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-09-03 17:34:16.097008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-03 17:34:16.097341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-03 17:34:16.099387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-03 17:34:16.100590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-09-03 17:34:16.105038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-03 17:34:16.105184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.106288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.107282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-03 17:34:16.109611: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 17:34:16.110732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.112321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2021-09-03 17:34:16.112394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-03 17:34:16.112443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-03 17:34:16.112488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-09-03 17:34:16.112534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-03 17:34:16.112580: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-03 17:34:16.112627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-03 17:34:16.112675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-09-03 17:34:16.112723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-03 17:34:16.112854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.114504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.115569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-03 17:34:16.115634: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-03 17:34:16.473167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-03 17:34:16.473188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-09-03 17:34:16.473192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-09-03 17:34:16.473331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.473802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.474240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 17:34:16.474662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 22460 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2021-09-03 17:34:16.474856: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cba9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "domaindata=pd.read_csv('Data/domaincomments.csv')\n",
    "domaindata=domaindata.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b387efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"jplu/tf-xlm-roberta-base\"##xlm-roberta-base #our model here\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0984d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"jplu/tf-xlm-roberta-large\"##xlm-roberta-base #our model here\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a966f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data/'\n",
    "\n",
    "lankadeepa_data_path = folder_path + 'lankadeepa_tagged_comments.csv'\n",
    "gossip_lanka_data_path = folder_path + 'gossip_lanka_tagged_comments.csv'\n",
    "lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]#just the end of document\n",
    "gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
    "gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
    "\n",
    "all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
    "#all_data=all_data[all_data['label']!=5]\n",
    "#all_data.shape\n",
    "\n",
    "#small_data=all_data[1000:]\n",
    "#small_data2=all_data[:1000]\n",
    "#small_data2=small_data2.reset_index(drop=True)\n",
    "\n",
    "#bal_data_2=small_data.loc[small_data['label']==2].sample(n=2000,random_state=42)#1500\n",
    "#bal_data_3=small_data.loc[small_data['label']==3].sample(n=1966,random_state=42)#\n",
    "#bal_data_4=small_data.loc[small_data['label']==4].sample(n=2000,random_state=42)#\n",
    "#bal_data_5=small_data.loc[small_data['label']==5].sample(n=1890,random_state=42)#\n",
    "\n",
    "#bal_data=pd.concat([bal_data_2,bal_data_3,bal_data_4,bal_data_5],ignore_index=True)#\n",
    "#bal_data=bal_data.sample(frac=1)\n",
    "#small_data=small_data.reset_index(drop=True)\n",
    "#bal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc2fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.read_csv('Data/newssource2.csv').sample(frac=1)#news source heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d37d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['comment'])\n",
    "    labels = list(data['label'])\n",
    "   \n",
    "    comments_splitted = []\n",
    "    #labels_rec=labels\n",
    "    #for comment in comments:\n",
    "    #    lines = []\n",
    "    #    try:\n",
    "    #        words = comment.split()\n",
    "    #        lines += words\n",
    "    #    except:\n",
    "    #        continue\n",
    "    #    comments_splitted.append(lines)\n",
    "    for i in range(len(labels)): \n",
    "        if labels[i]==2:#n\n",
    "            labels[i]=0\n",
    "            #print('0')\n",
    "        elif labels[i]==3:#neu\n",
    "            labels[i]=1##################\n",
    "            #print('3')\n",
    "        elif labels[i]==4:#p,(neu)\n",
    "            labels[i]=2\n",
    "            #print('2')\n",
    "        elif labels[i]==5:#con\n",
    "            labels[i]=3\n",
    "            #print('1')\n",
    "  \n",
    "  #labels should be starting from 0\n",
    "    return comments,labels\n",
    "\n",
    "def text_preprocessing2(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['comments'])\n",
    "    labels = list(data['labels'])#labels\n",
    "    \n",
    "    return comments,labels\n",
    "\n",
    "def text_preprocessing3(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['heads'])\n",
    "    labels = list(data['labels'])#labels\n",
    "    \n",
    "    return comments,labels\n",
    "\n",
    "def text_preprocessing5(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['comment'])\n",
    "    labels = list(data['label'])\n",
    "   \n",
    "   \n",
    "    for i in range(len(labels)): \n",
    "        if labels[i]=='negative':#n\n",
    "            labels[i]=0\n",
    "            #print('0')\n",
    "        elif labels[i]=='neutral':#p\n",
    "            labels[i]=1\n",
    "            #print('3')\n",
    "        elif labels[i]=='positive':#neu\n",
    "            labels[i]=2\n",
    "            #print('2')\n",
    "       \n",
    "    return comments,labels\n",
    "\n",
    "def text_preprocessing4(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['Tweets'])\n",
    "    labels = list(data['Emotion Set'])\n",
    "   \n",
    "    comments_splitted = []\n",
    "    #labels_rec=labels\n",
    "    #for comment in comments:\n",
    "    #    lines = []\n",
    "    #    try:\n",
    "    #        words = comment.split()\n",
    "    #        lines += words\n",
    "    #    except:\n",
    "    #        continue\n",
    "    #    comments_splitted.append(lines)\n",
    "    for i in range(len(labels)): \n",
    "        if labels[i]=='Sadness':#n\n",
    "            labels[i]=0\n",
    "            #print('0')\n",
    "        elif labels[i]=='Anger':#p\n",
    "            labels[i]=1\n",
    "            #print('1')\n",
    "        elif labels[i]=='Fear':#neu\n",
    "            labels[i]=2\n",
    "            #print('2')\n",
    "        elif labels[i]=='Disgust':#con\n",
    "            labels[i]=3\n",
    "            #print('3')\n",
    "        elif labels[i]=='Surprise':#con\n",
    "            labels[i]=4\n",
    "            #print('3')\n",
    "        elif labels[i]=='Happy':#con\n",
    "            labels[i]=5\n",
    "            #print('3')\n",
    "            \n",
    "  \n",
    "  #labels should be starting from 0\n",
    "    return comments,labels\n",
    "    \n",
    "def text_preprocessing8(data):\n",
    "    comments=[]\n",
    "    labels=[]\n",
    "    comments = list(data['comments'])\n",
    "    labels = list(data['labels'])\n",
    "   \n",
    "   \n",
    "    for i in range(len(labels)): \n",
    "        if labels[i]=='BLOG':#n\n",
    "            labels[i]=0\n",
    "            #print('0')\n",
    "        elif labels[i]=='CREATIVE':#p\n",
    "            labels[i]=1\n",
    "            #print('3')\n",
    "        elif labels[i]=='ACADEMIC':#neu\n",
    "            labels[i]=2            #print('2')\n",
    "        elif labels[i]=='NEWS':#neu\n",
    "            labels[i]=3\n",
    "            #print('2')\n",
    "       \n",
    "    return comments,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb578b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws='Data/writesty3.csv'\n",
    "all_data=pd.read_csv(ws)\n",
    "#all_data=all_data[all_data['labels']!='CREATIVE']\n",
    "\n",
    "#comments,labels=text_preprocessing8(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e7c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dzsize(n,data):\n",
    "    ws=['ACADEMIC','NEWS','CREATIVE','BLOG']\n",
    "    dz=pd.DataFrame()\n",
    "    for i in ws:#range(6):\n",
    "        f=int(np.floor(n/4))\n",
    "        dz=dz.append(((all_data[all_data['labels']==i]).sample(frac=1)).iloc[:f])\n",
    "        #print(i)\n",
    "    \n",
    "    return dz\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a7bddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dzsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_168529/2206466842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdz1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdzsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdz1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dzsize' is not defined"
     ]
    }
   ],
   "source": [
    "dz1=dzsize(11000,all_data)\n",
    "dz1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d176383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#comment_texts, comment_labels = text_preprocessing(small_data)\n",
    "##arr =comment_labels.values\n",
    "##comment_texts, comment_labels = text_preprocessing(bal_data)\n",
    "#comment_texts2, comment_labels2 = text_preprocessing(small_data2)\n",
    "\n",
    "comment_texts_all, comment_labels_all=text_preprocessing8(all_data)#(d)#\n",
    "#comment_texts_all2, comment_labels_all2=text_preprocessing5(all_data2)\n",
    "#comment_texts_all, comment_labels_all=text_preprocessing(dz)\n",
    "#comment_texts_all2,comment_labels_all2=text_preprocessing(sentedit[1506:11506])\n",
    "\n",
    "\n",
    "#comment_texts_all=comment_texts_all+comment_texts_all2\n",
    "#comment_labels_all=comment_labels_all+comment_labels_all2\n",
    "#comment_texts_all, comment_labels_all=text_preprocessing2(domaindata)\n",
    "\n",
    "#comment_texts_all, comment_labels_all=text_preprocessing4(emotiondata)\n",
    "\n",
    "\n",
    "# we then initialize the zero array\n",
    "#labels = np.zeros((len(small_data), arr.max()+1))\n",
    "\n",
    "# set relevant index for each row to 1 (one-hot encode)\n",
    "#labels[np.arange(len(small_data)), arr] = 1\n",
    "#print(labels)\n",
    "#comment_labels=np.array(comment_labels)\n",
    "#comment_texts=np.array(comment_texts)\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#comment_labels=labels\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(comment_texts, comment_labels, test_size=0.1, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(comment_texts_all, comment_labels_all, test_size=0.2, random_state=0)#0\n",
    "#X_train2, X_test2, y_train2, y_test2 = train_test_split(comment_texts_all2, comment_labels_all2, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3206ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.set_truncation_and_padding(400,MAX_LENGTH,LONGEST_FIRST,0,0)\n",
    "train_encodings = tokenizer(X_train,truncation=True,padding='max_length',max_length=400,return_tensors='np')#padding='max_length',max_length=512,\n",
    "#train_encodings = tokenizer(comment_texts_all, truncation=True, padding=True,return_tensors='np')\n",
    "#val_encodings = tokenizer(comment_texts2, truncation=True, padding=True, return_tensors='np')\n",
    "test_encodings = tokenizer(X_test, truncation=True,padding='max_length',max_length=400,return_tensors='np')#padding='max_length',max_length=512,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96cb083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:18:18.196849: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-10 12:18:18.197363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-10 12:18:18.264088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.264599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-01-10 12:18:18.264651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-10 12:18:18.265906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-10 12:18:18.265940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-10 12:18:18.266935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-10 12:18:18.267081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-10 12:18:18.268088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-10 12:18:18.268534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-10 12:18:18.270487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-10 12:18:18.270571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.271110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.271575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-01-10 12:18:18.271837: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-10 12:18:18.272186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.272659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Quadro RTX 6000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2022-01-10 12:18:18.272679: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-10 12:18:18.272692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-10 12:18:18.272703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-10 12:18:18.272713: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-10 12:18:18.272723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-10 12:18:18.272734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-10 12:18:18.272745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-10 12:18:18.272756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-10 12:18:18.272792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.273281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.273738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-01-10 12:18:18.273760: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-10 12:18:18.626094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-10 12:18:18.626113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-01-10 12:18:18.626117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-01-10 12:18:18.626258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.626734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.627173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 12:18:18.627614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22460 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2022-01-10 12:18:18.627795: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(( # convert to dataset objects\n",
    "    dict(train_encodings),\n",
    "    (np.array(y_train))#.astype(int)#tf.convert_to_tensor\n",
    "))\n",
    "#val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#    dict(test_encodings),####################\n",
    "#    np.array(y_test)\n",
    "#))\n",
    "#test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#    dict(val_encodings),\n",
    "#    np.array(comment_labels2)\n",
    "#))\n",
    "\n",
    "#test_emb=tf.data.Dataset.from_tensor_slices(( # convert to dataset objects\n",
    "#    dict(train_encodings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b4b8092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, ()), types: ({input_ids: tf.int64, attention_mask: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368b73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:18:22.144212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFXLMRobertaForSequenceClassification were not initialized from the model checkpoint at jplu/tf-xlm-roberta-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, TFTrainingArguments, TFTrainer\n",
    "num_labels=4 #5 #4 #6 #3\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08595297",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)#SparseCategoricalCrossentropy\n",
    "optimizeradam = tf.keras.optimizers.Adam(learning_rate=0.000005)#0.000005\n",
    "\n",
    "loss2=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "step = tf.Variable(0, trainable=False)\n",
    "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [600], [5e-6, 1e-6])#-6#500#1500#700\n",
    "   # lr and wd can be a function or a tensor\n",
    "lr = 1 * schedule(step)\n",
    "wd = lambda: 1e-6 * schedule(step)\n",
    "\n",
    "schedule2=tf.optimizers.schedules.PolynomialDecay(5e-6,800,end_learning_rate=5e-7)#5e-7\n",
    "lr2=1*schedule2(step)\n",
    "wd2 = lambda: 1e-6 * schedule(step)\n",
    "\n",
    "optimizer=AdamW(learning_rate=lr,weight_decay=wd)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])#model.compute_loss# can also use any keras loss fn, loss=model.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfdc6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#    [500], [5e-6, 1e-6])\n",
    "   # lr and wd can be a function or a tensor\n",
    "#lr = 1 * schedule(step)\n",
    "#wd = lambda: 1e-6 * schedule(step)\n",
    "#len(dict(train_encodings)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc7a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fea387277c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fea387277c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:18:37.640801: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-10 12:18:37.776218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1252/1252 [==============================] - 1188s 934ms/step - loss: 0.5854 - accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "1252/1252 [==============================] - 1170s 935ms/step - loss: 0.0674 - accuracy: 0.9797\n",
      "Epoch 3/5\n",
      "1252/1252 [==============================] - 1170s 935ms/step - loss: 0.0454 - accuracy: 0.9875\n",
      "Epoch 4/5\n",
      "1252/1252 [==============================] - 1170s 934ms/step - loss: 0.0266 - accuracy: 0.9934\n",
      "Epoch 5/5\n",
      "1252/1252 [==============================] - 1170s 935ms/step - loss: 0.0209 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "his=model.fit(train_dataset.shuffle(1000).batch(8),epochs=5)#dict(train_encodings),np.array(y_train),epochs=5,batch_size=16 # #callbacks[history]#train_dataset.shuffle(1000).batch(16) #epochs=5,4,#validation_data=val_dataset.shuffle(1000).batch(16),callbacks=[es],dict(train_encodings),np.array(y_train).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff4abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9859    0.9813    0.9836       427\n",
      "           1     0.9832    0.9936    0.9884       472\n",
      "           2     0.9971    0.9943    0.9957       703\n",
      "           3     0.9944    0.9933    0.9939       901\n",
      "\n",
      "    accuracy                         0.9916      2503\n",
      "   macro avg     0.9902    0.9906    0.9904      2503\n",
      "weighted avg     0.9916    0.9916    0.9916      2503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out=model.predict(dict(test_encodings))#dict(val_encodings),dict(test_encodings),dat2,new_test_input\n",
    "y_pred=np.array((np.argmax((tf.nn.softmax(out.logits,axis=-1)),axis=-1)))\n",
    "print(classification_report(np.array(y_test),np.array(y_pred),digits=4))#comment_labels2,y_test\n",
    "rep=classification_report(np.array(y_test),np.array(y_pred),digits=4)\n",
    "#with open('result_4sent_2.txt','w+') as f:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv]",
   "language": "python",
   "name": "conda-env-tfenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
